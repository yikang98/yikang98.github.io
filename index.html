<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
    <link rel="shortcut icon" href="myIcon.ico">
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

    <meta name="keywords" content="Yikang Ding">
    <meta name="description" content="Yikang Ding&#39;s home page">
    <meta name="google-site-verification" content="X2QFrl-bPeg9AdlMt4VKT9v6MJUSTCf-SrY3CvKt4Zs" />
    <link rel="stylesheet" href="jemdoc.css" type="text/css">
    <title>Yikang Ding&#39;s Homepage</title>
    <!-- Google Analytics -->
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-159069803-1', 'auto');
        ga('send', 'pageview');
    </script>
    <!-- End Google Analytics -->
    <!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
-->
</head>

<body>
    <div id="layout-content" style="margin-top:25px">
        <table>
            <tbody>
                <tr>
                    <td width="650">
                        <div id="toptitle">
                            <h1>Yikang Ding -
                                <font face="Arial"> 丁宜康 </font>
                            </h1>
                        </div>

                        <!-- <h3>Mphil Candidate</h3> -->
                        <p>
                            Email: dingyikang23@gmail.com <br>
                            WeChat: yeecon_ <br>
                            Phone: +86 156 5258 3576 <br>
                            <!-- Electronic and Information Engineering <br> Tsinghua University <br> Beijing, China<br>
                            <br> Email: dyk20@mails.tsinghua.edu.cn -->
                            <h4><a href="https://github.com/DingYikang/">[GitHub]</a> <a href="https://scholar.google.com/citations?user=gdP9StQAAAAJ&hl=zh-CN&oi=ao/">[Google Scholar]</a> </h4>
                        </p>
                        </p>
                    </td>
                    <!-- <td>
                        <img src="./pic/person.jpg" border="0" width="220"><br>
                    </td> -->
                </tr>
                <tr>
                </tr>
            </tbody>
        </table>

        <h2>Biography </h2>
        <p>
            I am currently a researcher in Megvii, where I am working on the task of 3D reconstruction, world model and generative model.
            Before that, I got my master's degree and bachelor's degree at Tsinghua University and Beihang University.
            If you have any interest, please feel free to contact me.
        </p>
        <h2>News</h2>
        <ul>
            <li>
                [02/2025] <b>One paper</b> is accepted to CVPR 2025.
            </li>
            <li>
                [07/2024] <b>One paper</b> is accepted to ECCV 2024 as <b style="color: red">Oral</b>.
            </li>
            <li>
                [03/2023] Win the <b style="color: red">Innovation Award</b> of 3D Occupancy Prediction Challege in CVPR 2023.
            </li>
            <li>
                [03/2023] <b>One paper</b> is accepted to CVPR 2023.
            </li>
        </ul>
        <details>
            <summary>More news...</summary>
            <ul>
                <li>
                    [09/2022] <b>One paper</b> is accepted to NeurIPS 2022.
                </li>
                <li>
                    [06/2022] <b>Two papers</b> are accepted to ECCV 2022.
                </li>
                <li>
                    [03/2022] <b>One paper</b> is accepted to CVPR 2022.
                </li>
            </ul>
        </details>



        <h2>
            <font>Selected Publications </font>
        </h2>
        <p>
            (* denotes equal contribution, <sup>✝</sup> denotes project leader.)
        </p>
        <table id="tbPublications" width="100%">
            <tbody>
                <tr>
                    <td width="206">
                        <img src="pic/2025/dist_ppl.png" width="185px" height="95" style="box-shadow: 4px 4px 8px #888">
                    </td>
                    <td><b><a href="">DiST-4D: Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation</a></b> <br>
                        <font color="gray">Jiazhe Guo*, </font><b>Yikang Ding*<sup>✝</sup></b> <font color="gray">, Xiwu Chen, Shuo Chen, Bohan Li, Yingshuang Zou, Xiaoyang Lyu, Feiyang Tan, Xiaojuan Qi, Zhiheng Li, Hao Zhao</font><br>
                        <p><p><b>Arxiv Preprint 2025</b><br> [
                            <a href="">paper</a> ] [ <a href="https://royalmelon0505.github.io/DiST-4D/">project page</a> ] [ <a href="https://github.com/royalmelon0505/dist4d">code</a> ]
                    </td>
                </tr>
                <tr>
                    <td width="206">
                        <img src="pic/2025/mudg.jpg" width="185px" height="95" style="box-shadow: 4px 4px 8px #888">
                    </td>
                    <td><b><a href="">MuDG: Taming Multi-modal Diffusion with Gaussian Splatting for Urban Scene Reconstruction</a></b> <br>
                        <font color="gray">Yingshuang Zou*, </font><b>Yikang Ding*<sup>✝</sup></b> <font color="gray">, Chuanrui Zhang, Jiazhe Guo, Bohan Li, Xiaoyang Lyu, Feiyang Tan, Xiaojuan Qi, Haoqian Wang</font><br>
                        <p><p><b>Arxiv Preprint 2025</b><br> [
                            <a href="https://arxiv.org/pdf/2503.10604">paper</a> ] [ <a href="https://heiheishuang.xyz/mudg/">project page</a> ] [ <a href="https://github.com/heiheishuang/MuDG">code</a> ]
                    </td>
                </tr>
                <tr>
                    <td width="206">
                        <img src="pic/2024/hermes.png" width="185px" height="95" style="box-shadow: 4px 4px 8px #888">
                    </td>
                    <td><b><a href="">HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation</a></b> <br>
                        <font color="gray">Xin Zhou*, Dingkang Liang*<sup>✝</sup>, Sifan Tu, Xiwu Chen, </font><b>Yikang Ding<sup>✝</sup></b> <font color="gray">, Dingyuan Zhang, Feiyang Tan, Hengshuang Zhao, Xiang Bai</font><br>
                        <p><p><b>Arxiv Preprint 2025</b><br> [
                            <a href="https://arxiv.org/pdf/2501.14729">paper</a> ] [ <a href="https://lmd0311.github.io/HERMES/">project page</a> ] [ <a href="https://github.com/LMD0311/HERMES">code</a> ]
                    </td>
                </tr>
                <tr>
                    <td width="206">
                        <img src="pic/2024/uniscene.png" width="185px" height="95" style="box-shadow: 4px 4px 8px #888">
                    </td>
                    <td><b><a href="">UniScene: Unified Occupancy-centric Driving Scene Generation</a></b> <br>
                        <font color="gray">Bohan Li*, Jiazhe Guo*, Hongsi Liu*, Yingshuang Zou*, </font><b>Yikang Ding*<sup>✝</sup></b> <font color="gray">, Xiwu Chen, Hu Zhu, Other authors</font><br>
                        <p><p>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2025</b>)<br> [
                            <a href="https://arxiv.org/pdf/2412.05435">paper</a> ] [ <a href="https://arlo0o.github.io/uniscene/">project page</a> ] [ <a href="https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation">code</a> ]
                    </td>
                </tr>
                <tr>
                    <td width="206">
                        <img src="pic/2024/m2depth.png" width="185px" height="95" style="box-shadow: 4px 4px 8px #888">
                    </td>
                    <td><b><a href="">M2Depth: Self-supervised Two-Frame Multi-camera Metric Depth Estimation</a></b> <br>
                        <font color="gray">Yingshuang Zou*, </font><b>Yikang Ding*<sup>✝</sup></b> <font color="gray">, Xi Qiu, Haoqian Wang, Haotian Zhang</font><br>
                        <p><p>European Conference on Computer Vision (<b>ECCV 2024</b> <b style="color: red">Oral</b>)<br> [
                            <a href="https://arxiv.org/pdf/2405.02004">paper</a> ] [ <a href="https://heiheishuang.xyz/M2Depth/">project page</a> ]
                    </td>
                </tr>
                <tr>
                    <td width="206">
                        <img src="pic/2022/adamatcher.jpg" width="185px" height="95" style="box-shadow: 4px 4px 8px #888">
                    </td>
                    <td><b><a href="">Adaptive Assignment for Geometry Aware Local Feature Matching</a></b> <br>
                        <font color="gray">Dihe Huang, Ying Chen, Shang Xu, Yong Liu, Wenlong Wu, </font><b>Yikang Ding</b>
                        <font color="gray">, Chengjie Wang, Fan Tang</font><br>
                        <p><p>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2023</b>)<br> [
                            <a href="https://arxiv.org/pdf/2207.08427">paper</a> ] [ <a href="https://github.com/TencentYoutuResearch/AdaMatcher">code</a> ]
                    </td>
                </tr>
                <tr>
                    <td width="206">
                        <img src="pic/2022/transmvsnet.jpg" width="185px" height="95" style="box-shadow: 4px 4px 8px #888">
                    </td>
                    <td><b><a href="">TransMVSNet: Global Context-aware Multi-view Stereo Network with Transformers</a></b> <br>
                        <b>Yikang Ding*</b>
                        <font color="gray">, Wentao Yuan*, Qingtian Zhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, Xiao Liu</font><br>
                        <p>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2022</b>)<br> [
                            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_TransMVSNet_Global_Context-Aware_Multi-View_Stereo_Network_With_Transformers_CVPR_2022_paper.pdf">paper</a> ] [
                            <a href="https://github.com/megvii-research/TransMVSNet">code</a> ] [
                            <a href="https://dingyikang.github.io/transmvsnet.github.io/">project page</a> ] [
                            <a href="https://www.bilibili.com/video/BV15S4y1q7mE?spm_id_from=333.999.0.0&vd_source=72cff359c64c313c1d3741ba33b2d910">tech talk (<font face="Arial">中文</font>)</a> ]
                    </td>
                </tr>
                <tr>
                    <td width="206">
                        <img src="pic/2022/kdmvs.jpg" width="185px" height="95" style="box-shadow: 4px 4px 8px #888">
                    </td>
                    <td><b><a href="">KD-MVS: Knowledge Distillation Based Self-supervised Learning for Multi-view Stereo</a></b> <br>
                        <b>Yikang Ding</b>
                        <font color="gray">, Qingtian Zhu, Xiangyue Liu, Wentao Yuan, Haotian Zhang, Chi Zhang</font><br>
                        <p>European Conference on Computer Vision (<b>ECCV 2022</b>)<br> [
                            <a href="https://arxiv.org/abs/2207.10425">paper</a> ] [
                            <a href="https://github.com/megvii-research/KD-MVS">code</a> ] [
                            <a href="https://dingyikang.github.io/kdmvs.github.io/">project page</a> ]
                    </td>
                </tr>
                <tr>
                    <td width="206">
                        <img src="pic/2024/layerdiffusion.png" width="185px" height="95" style="box-shadow: 4px 4px 8px #888">
                    </td>
                    <td><b><a href="">LayerDiffusion: Layered Controlled Image Editing with Diffusion Models</a></b> <br>
                        <font color="gray">Pengzhi Li, Qinxuan Huang, </font><b>Yikang Ding</b><font color="gray">, Zhiheng Li</font><br>
                        <p>SIGGRAPH Asia 2023 Technical Communications (<b>SIGGRAPH Asia 2023</b>)<br> [
                            <a href="https://dl.acm.org/doi/pdf/10.1145/3610543.3626172">paper</a> ]
                    </td>
                </tr>
                <tr>
                    <td width="206">
                        <img src="pic/2022/wtmvs.jpg" width="185px" height="95" style="box-shadow: 4px 4px 8px #888">
                    </td>
                    <td><b><a href="">WT-MVSNet: Window-based Transformers for Multi-view Stereo</a></b> <br>
                        <font color="gray">Jinli Liao*,</font> <b>Yikang Ding*</b>
                        <font color="gray">, Yoli Shavit, Dihe Huang, Shihao Ren, Jia Guo, Wensen Feng, Kai Zhang</font><br>
                        <p>Thirty-sixth Conference on Neural Information Processing Systems (<b>NeurIPS 2022</b>)<br> [
                            <a href="https://arxiv.org/pdf/2205.14319">paper</a> ]
                    </td>
                </tr>
                <tr>
                    <td width="206">
                        <img src="pic/2022/soblev.jpg" width="185px" height="95" style="box-shadow: 4px 4px 8px #888">
                    </td>
                    <td><b><a href="">Sobolev Training for Implicit Neural Representations with Approximated Image Derivatives</a></b> <br>
                        <font color="gray">Wentao Yuan, Qingtian Zhu, Xiangyue Liu,</font> <b>Yikang Ding</b>
                        <font color="gray">, Haotian Zhang, Chi Zhang</font><br>
                        <p>European Conference on Computer Vision (<b>ECCV 2022</b>)<br> [
                            <a href="https://arxiv.org/abs/2207.10395">paper</a> ] [
                            <a href="https://github.com/megvii-research/Sobolev_INRs">code</a> ]
                    </td>
                </tr>
                <!-- <tr>
                    <td width="206">
                        <img src="pic/2022/icra.gif" width="185px" height="95" style="box-shadow: 4px 4px 8px #888">
                    </td>
                    <td><b><a href="">Rethinking Dimensionality Reduction in Grid-based 3D Object Detection</a></b> <br>
                        <font color="gray">Dihe Huang, Ying Chen, </font><b>Yikang Ding</b>
                        <font color="gray">, Jinli Liao, Jianlin Liu, Kai Wu, Qiang Nie, Yong Liu</font><br>
                        <p>Arxiv Preprint<br> [
                            <a href="">paper</a> ] [ code ]
                    </td>
                </tr> -->
                

                <tr></tr>
                <tr></tr>
                <tr></tr>

            </tbody>
        </table>
        <h2>
            <font>Honors & Awards </font>
        </h2>
        <ul>
            <li>
                <b>Innovation Award</b> of 3D Occupancy Prediction Challenge in CVPR 2023, 2023.03
            </li>
            <li>
                <b>1st place</b> in Indoor and Outdoor Visual Localization Challenge of ICCV 2021 Workshop on Long-term Visual Localization under Changing Conditions, 2021.10 [<a href="https://sites.google.com/view/ltvl2021/">website</a>] [<a href="https://www.youtube.com/watch?v=BTDrD0ljduI">talk</a>]
            </li>
            <li>
                Outstanding Graduate of Beijing, 2020.06
            </li>
            <li>
                <i>May-4th</i> Medal of Beihang University (10 people per year), 2019.11
            </li>
            <li>
                <i>Student-of-the-Year</i> of Beihang University (10 people per year), 2019.11
            </li>
            <li>
                AVIC Academic Scholarship of Beihang University (10 people per year), 2019.11
            </li>
        </ul>



        <h2>
            <font>Academic Services </font>
        </h2>
        <ul style="list-style-type:none">
            <li>
                <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;">
                    <font size="3">
                        <meta charset="utf-8">
                        <strong> Conference Reviewer: </strong> <br> </font>
                </p>
                <p style="margin-left: 20px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;">
                    <font size="3">
                        <meta charset="utf-8"> CVPR 2022-, ECCV 2022-, AAAI 2023-, ACCV 2022-, 3DV 2022-
                    </font>
                </p>
            </li>
        </ul>
        <!-- <p align=right>
            <a class="pull-right" href="#">
                <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/globe.js?d=sjN32DQJNg-ersu1omfeQGDEj9WdB2f6k0I6ZmtjVSI"></script>
                <style>
                    .clustrmaps-map-container canvas {  /* 修改选择器 */
                        width: 150px !important;
                        height: 150px !important;
                    }
                </style>
            </a>
        </p> -->

        <p>
            <center>
                <font>
                    <br>&copy; Yikang Ding | Last updated: Mar. 2025</font>
            </center>
        </p>

</body>

</html>